from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os
from groq import Groq
from dotenv import load_dotenv
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

app = FastAPI()

# 1. GROQ_API_KEY ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ìƒì„±
# ë Œë” í™˜ê²½ë³€ìˆ˜ë‚˜ .env íŒŒì¼ì— GROQ_API_KEYë¥¼ ê¼­ ë„£ì–´ì£¼ì„¸ìš”!
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if GROQ_API_KEY:
    client = Groq(api_key=GROQ_API_KEY)
    logger.info("âœ… Groq API Keyê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. (Llama 3 í™œì„±í™” ì™„ë£Œ)")
else:
    logger.error("âŒ Groq API Keyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    client = None

# 2. í˜ë¥´ì†Œë‚˜ ë¡œë“œ
current_dir = os.path.dirname(os.path.abspath(__file__))
persona_path = os.path.join(current_dir, "persona.txt")

try:
    with open(persona_path, "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()
    logger.info("âœ… í˜ë¥´ì†Œë‚˜ íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì „ë¬¸ í—¬ìŠ¤ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤."
    logger.warning("âš ï¸ persona.txtë¥¼ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

class ChatRequest(BaseModel):
    user_id: str
    message: str
    context: str = ""

@app.get("/")
def read_root():
    return {"status": "online", "message": "Gains & Guide AI Coach Server (Llama 3) is Running! ğŸ‹ï¸â€â™‚ï¸"}

@app.post("/chat")
async def chat_with_coach(request: ChatRequest):
    if not client:
        logger.error("API Key ë¯¸ì„¤ì • ìƒíƒœ")
        raise HTTPException(status_code=500, detail="ì„œë²„ì— Groq API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    try:
        logger.info(f"ìš”ì²­ ìˆ˜ì‹  - User: {request.user_id}, Message: {request.message[:20]}...")

        # 3. Groq (Llama 3) í˜•ì‹ì— ë§ì¶° ë©”ì‹œì§€ ì¡°ë¦½
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"[ê³¼ê±° ìš´ë™ ê¸°ë¡]\n{request.context}\n\n[ì§ˆë¬¸]\n{request.message}"}
        ]

        chat_completion = client.chat.completions.create(
            messages=messages,
            model="llama-3.1-8b-instant", # ğŸ‘ˆ "llama3-70b-8192" ëŒ€ì‹  ì´ ì´ë¦„ì„ ë„£ìœ¼ì„¸ìš”!
            temperature=0.7,
            max_tokens=1024,
        )

        reply = chat_completion.choices[0].message.content

        if not reply:
            return {"response": "AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

        return {"response": reply}

    except Exception as e:
        logger.exception("âŒ ë‹µë³€ ìƒì„± ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ:")
        raise HTTPException(status_code=500, detail=f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)